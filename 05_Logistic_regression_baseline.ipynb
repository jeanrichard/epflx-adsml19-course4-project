{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4 - Project - Part 5: Logistic regression baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top-5\"></a>\n",
    "This notebook is concerned with *Part 5: Logistic regression baseline*.\n",
    "\n",
    "**Contents:**\n",
    "* [Step 0: Loading data](#step-5.0)\n",
    "* [Step 1: Evaluate the logistic regression baseline](#step-5.1)\n",
    "* [Step 2: Tune its regularization strength parameter](#step-5.2)\n",
    "* [Step 3: Pick a few images and compute the probability for each class](#step-5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Loading data<a name=\"step-5.0\"></a> ([top](#top-5))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training set with the extracted high-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library.\n",
    "import os\n",
    "import pathlib\n",
    "import typing as T\n",
    "\n",
    "# 3rd party.\n",
    "import numpy as np\n",
    "\n",
    "# Project.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Dataset: train\n",
      "data: shape=(280, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(280,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(280,), dtype=<U19\n",
      "features: shape=(280, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: valid\n",
      "data: shape=(139, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(139,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(139,), dtype=<U19\n",
      "features: shape=(139, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: test\n",
      "data: shape=(50, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(50,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(50,), dtype=<U19\n",
      "features: shape=(50, 1280), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "separator = ''.center(80, '-')\n",
    "\n",
    "path_train = pathlib.Path.cwd() / 'data' / 'swissroads-features-train.npz'\n",
    "data_train = utils.load(path_train)\n",
    "print(separator)\n",
    "print(f'Dataset: train\\n{utils.info(data_train)}')\n",
    "\n",
    "path_valid = pathlib.Path.cwd() / 'data' / 'swissroads-features-valid.npz'\n",
    "data_valid = utils.load(path_valid)\n",
    "print(separator)\n",
    "print(f'Dataset: valid\\n{utils.info(data_valid)}')\n",
    "\n",
    "path_test = pathlib.Path.cwd() / 'data' / 'swissroads-features-test.npz'\n",
    "data_test = utils.load(path_test)\n",
    "print(separator)\n",
    "print(f'Dataset: test\\n{utils.info(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strs = data_train['label_strs']  # Same for all data sets.\n",
    "assert (\n",
    "    np.all(data_train['label_strs'] == data_valid['label_strs']) and\n",
    "    np.all(data_train['label_strs'] == data_test['label_strs'])\n",
    ")\n",
    "\n",
    "X_train = data_train['data']\n",
    "y_train = data_train['label_idxs']\n",
    "F_train = data_train['features']\n",
    "N_train = data_train['names']\n",
    "\n",
    "X_valid = data_valid['data']\n",
    "y_valid = data_valid['label_idxs']\n",
    "F_valid = data_valid['features']\n",
    "N_valid = data_train['names']\n",
    "\n",
    "X_test = data_test['data']\n",
    "y_test = data_test['label_idxs']\n",
    "F_test = data_test['features']\n",
    "N_test = data_test['names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Evaluate the logistic regression baseline<a name=\"step-5.1\"></a> ([top](#top-5))\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library.\n",
    "import warnings\n",
    "\n",
    "# 3rd party.\n",
    "import pandas as pd\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use logistic regression for a multiclass classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part it makes sense to merge the training and the validation sets since otherwise we would not make use of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_large = np.concatenate([X_train, X_valid])\n",
    "y_train_large = np.concatenate([y_train, y_valid])\n",
    "F_train_large = np.concatenate([F_train, F_valid])\n",
    "N_train_large = np.concatenate([N_train, N_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pipeline comprised of an optional scaler and a logistic regression classifier (to be fleshed out later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the estimator.\n",
    "logreg_pipe = Pipeline([\n",
    "    ('scaler', None),\n",
    "    ('logreg', None)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline, we decide to try:\n",
    "* Scaling: none and standard-scaler.\n",
    "* Strategies: one-vs-one (OVO), one-vs-rest (OVR) and softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0\n",
    "MAX_ITER = 200\n",
    "\n",
    "scalers = [None, StandardScaler()]\n",
    "\n",
    "# Define the grid of values.\n",
    "grid = ParameterGrid([\n",
    "    # OVO\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            OneVsOneClassifier(\n",
    "                LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "            )\n",
    "        ],\n",
    "        'logreg__estimator__solver': ['liblinear']\n",
    "    },\n",
    "    # OVR\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "        ],\n",
    "        'logreg__solver': ['liblinear'],\n",
    "        'logreg__multi_class': ['ovr']\n",
    "    },\n",
    "    # Softmax\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "        ],\n",
    "        'logreg__solver': ['saga'],\n",
    "        'logreg__max_iter': [MAX_ITER],\n",
    "        'logreg__multi_class': ['multinomial']\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We had to work around a few warnings:\n",
    "* The default maximum number of iterations for Softmax causes a `ConvergenceWarning: The max_iter was reached which means the coef_ did not converge`. So we increased this parameter from 100 (default) to 200 and suppressed the remaining warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = []\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "\n",
    "    # Filter convergence warnings.\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "    for params_dict in grid:\n",
    "        # Set the parameters.\n",
    "        logreg_pipe.set_params(**params_dict)\n",
    "\n",
    "        # Fit the clasifier to the data.\n",
    "        logreg_pipe.fit(F_train_large, y_train_large)\n",
    "\n",
    "        # Store the scores.\n",
    "        gs_result = params_dict\n",
    "        gs_result['accuracy_train'] = logreg_pipe.score(F_train_large, y_train_large)\n",
    "        gs_result['accuracy_test'] = logreg_pipe.score(F_test, y_test)\n",
    "\n",
    "        # Save result.\n",
    "        gs_results.append(gs_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the results to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to a data frame.\n",
    "df_results = (pd\n",
    "              .DataFrame(gs_results)\n",
    "              .sort_values(by='accuracy_test', ascending=False)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler</th>\n",
       "      <th>strategy</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standardscaler</td>\n",
       "      <td>ovr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>ovr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standardscaler</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standardscaler</td>\n",
       "      <td>ovo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>ovo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           scaler     strategy  accuracy_train  accuracy_test\n",
       "3  standardscaler          ovr             1.0           0.98\n",
       "2            none          ovr             1.0           0.94\n",
       "5  standardscaler  multinomial             1.0           0.94\n",
       "1  standardscaler          ovo             1.0           0.92\n",
       "4            none  multinomial             1.0           0.92\n",
       "0            none          ovo             1.0           0.90"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make it more readable.\n",
    "df_report = df_results.copy()\n",
    "df_report['scaler'] = df_report['scaler'].apply(\n",
    "    lambda est: (str(est) if est is None else\n",
    "                 type(est).__name__).lower())\n",
    "df_report['strategy'] = df_report['logreg'].apply(\n",
    "    lambda est: ('ovo' if isinstance(est, OneVsOneClassifier) else\n",
    "                 est.get_params()['multi_class']).lower())\n",
    "\n",
    "df_report[['scaler', 'strategy', 'accuracy_train', 'accuracy_test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** So far, using a standard scaler and a logistic regression with OVR yields the best accuracy on the test set (98 %)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tune its regularization strength parameter<a name=\"step-5.2\"></a> ([top](#top-5))\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to tune the regularization strength of the logistic regression classifier with cross-validated grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0\n",
    "MAX_ITER = 200\n",
    "\n",
    "scalers = [StandardScaler()]  # This should help the solvers.\n",
    "\n",
    "# Using np.logspace(a, b, num=k * (b - a) + 1), where a, b are integers and k > 0 is an integer, we\n",
    "# include integer exponents in the range [a, b]. \n",
    "Cs = np.logspace(-4, 4, num=3 * 8 + 1)  # C defaults to 1.0.\n",
    "\n",
    "grid = [\n",
    "    # OVO\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            OneVsOneClassifier(\n",
    "                LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "            )\n",
    "        ],\n",
    "        'logreg__estimator__C': Cs,\n",
    "        'logreg__estimator__solver': ['liblinear']\n",
    "    },\n",
    "    # OVR\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "        ],\n",
    "        'logreg__C': Cs,\n",
    "        'logreg__solver': ['liblinear'],\n",
    "        'logreg__multi_class': ['ovr']\n",
    "    },\n",
    "    # Softmax\n",
    "    {\n",
    "        'scaler': scalers,\n",
    "        'logreg': [\n",
    "            LogisticRegression(random_state=RANDOM_STATE)  # Make it deterministic.\n",
    "        ],\n",
    "        'logreg__C': Cs,\n",
    "        'logreg__solver': ['saga'],\n",
    "        'logreg__max_iter': [MAX_ITER],\n",
    "        'logreg__multi_class': ['multinomial']\n",
    "    }\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=RANDOM_STATE)\n",
    "logreg_gscv = GridSearchCV(logreg_pipe, grid, n_jobs=-1, iid=False, refit=False, cv=cv, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "\n",
    "    # Filter convergence warnings.\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "    # Fit/evaluate the estimator.\n",
    "    logreg_gscv.fit(F_train_large, y_train_large);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results in a data frame.\n",
    "df_results = (pd\n",
    "    .DataFrame({\n",
    "        'strategy': np.where(~logreg_gscv.cv_results_['param_logreg__multi_class'].mask,\n",
    "                              logreg_gscv.cv_results_['param_logreg__multi_class'], 'ovo'),\n",
    "        'C': np.where(~logreg_gscv.cv_results_['param_logreg__C'].mask,\n",
    "                       logreg_gscv.cv_results_['param_logreg__C'],\n",
    "                       logreg_gscv.cv_results_['param_logreg__estimator__C']),\n",
    "        'mean_train': logreg_gscv.cv_results_['mean_train_score'],\n",
    "        'mean_test': logreg_gscv.cv_results_['mean_test_score'],\n",
    "        'std_test': logreg_gscv.cv_results_['std_test_score'],\n",
    "        'params': logreg_gscv.cv_results_['params']\n",
    "    })\n",
    "    .sort_values(by='mean_test', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best results for one-vs-one, one-vs-rest and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 10 best combinations for one-vs-one, one-vs-rest and softmax (according to the mean test score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[['strategy', 'C', 'mean_train', 'mean_test', 'std_test']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The OVO strategy seems to yield the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the accuracy of the best model (among OVO, OVR and Softmax) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "\n",
    "    # Filter convergence warnings.\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "    params_best = df_results.iloc[0]['params']\n",
    "    logreg_pipe.set_params(**params_best)\n",
    "    logreg_pipe.fit(F_train_large, y_train_large)\n",
    "    accuracy_train_large = logreg_pipe.score(F_train_large, y_train_large)\n",
    "    accuracy_test = logreg_pipe.score(F_test, y_test)\n",
    "    print(f'training accuracy: {accuracy_train_large * 100:.1f} %')\n",
    "    print(f'test accuracy: {accuracy_test * 100:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best results for one-vs-rest and softmax only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are the 10 best combinations for one-vs-rest and softmax only (according to the mean test score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results['strategy'] != 'ovo'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "\n",
    "    # Filter convergence warnings.\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "    params_best = df_results[df_results['strategy'] != 'ovo'].iloc[0]['params']\n",
    "    logreg_pipe.set_params(**params_best)\n",
    "    logreg_pipe.fit(F_train_large, y_train_large)\n",
    "    accuracy_train_large = logreg_pipe.score(F_train_large, y_train_large)\n",
    "    accuracy_test = logreg_pipe.score(F_test, y_test)\n",
    "    print(f'training accuracy: {accuracy_train_large * 100:.1f} %')\n",
    "    print(f'test accuracy: {accuracy_test * 100:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pick a few images and compute the probability for each class<a name=\"step-5.3\"></a> ([top](#top-5))\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only one-vs-rest and softmax offer probability estimates, we put one-vs-one aside.\n",
    "\n",
    "We start by randomly picking 2 images in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSAMPLES = 2\n",
    "\n",
    "buf = []\n",
    "for label in range(len(label_strs)):\n",
    "    # Randomly choose indices of images that belong to this category.\n",
    "    (idxs,) = np.nonzero(y_test == label)\n",
    "    idxs_cat = np.random.choice(idxs, NSAMPLES, replace=False)\n",
    "    buf.append(idxs_cat)\n",
    "idxs_sample = np.concatenate(buf)\n",
    "idxs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_WIDTH = 1.75\n",
    "CELL_HEIGHT = 1.75\n",
    "\n",
    "# Make it deterministic.\n",
    "np.random.seed(0)\n",
    "\n",
    "nrows = 1\n",
    "ncols = len(label_strs) * NSAMPLES\n",
    "\n",
    "# Get the corresponding images, high-level features and names.\n",
    "X_sample = X_test[idxs_sample]\n",
    "F_sample = F_test[idxs_sample]  # shape=(1280,)\n",
    "F_sample = F_sample[np.newaxis, :]  # shape=(1, 1280)\n",
    "N_sample = N_test[idxs_sample]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols,\n",
    "                        figsize=(ncols * CELL_WIDTH, nrows * CELL_HEIGHT),\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "# Display the images.\n",
    "for j, (ax, idx) in enumerate(zip(axs, idxs_sample)):\n",
    "    ax.imshow(X_test[idx])\n",
    "    \n",
    "    # Add labels.\n",
    "    name_short = pathlib.Path(N_test[idx]).stem\n",
    "    ax.set_xlabel(f'{name_short}', fontsize=10)\n",
    "\n",
    "    # X/y-axis: hide the ticks and their labels.\n",
    "    ax.tick_params(axis='both', which='both', length=0)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "# Compute the probabilities.\n",
    "# params_best\n",
    "# logreg_pipe.set_params(**params_best)\n",
    "# logreg_pipe.fit(F_train_large, y_train_large)\n",
    "accuracy_train_large = logreg_pipe.score(F_train_large, y_train_large)\n",
    "accuracy_test = logreg_pipe.score(F_test, y_test)\n",
    "print(f'training accuracy: {accuracy_train_large * 100:.1f} %')\n",
    "print(f'test accuracy: {accuracy_test * 100:.1f} %')\n",
    "\n",
    "F_sample = F_test[idxs_sample]  # shape=(1280,)\n",
    "probas_sample = logreg_pipe.predict_proba(F_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned estimates for all classes are ordered by the label of classes.\n",
    "short_names = [pathlib.Path(N_test[idx]).stem for idx in idxs_sample]\n",
    "df = pd.DataFrame(data=probas_sample, index=short_names, columns=label_strs)\n",
    "\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html\n",
    "df.style.highlight_max(axis=1).format('{:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_min = Cs.min()\n",
    "C_max = Cs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_C(logreg_pipe, params, C, F_train, y_train, F_test, y_test, F_sample):\n",
    "    # Compute the probabilities.\n",
    "    params_C = params.copy()\n",
    "    params_C['logreg__C'] = C\n",
    "    params_C['logreg__max_iter'] = 1000\n",
    "    logreg_pipe.set_params(**params_C)\n",
    "    logreg_pipe.fit(F_train, y_train)\n",
    "    \n",
    "    accuracy_train = logreg_pipe.score(F_train, y_train)\n",
    "    accuracy_test = logreg_pipe.score(F_test, y_test)\n",
    "    print(f'C={C}')\n",
    "    print(f'training accuracy: {accuracy_train:.2%}')\n",
    "    print(f'test accuracy: {accuracy_test:.2%}')\n",
    "    \n",
    "    probas_sample = logreg_pipe.predict_proba(F_sample)\n",
    "    # The returned estimates for all classes are ordered by the label of classes.\n",
    "    short_names = [pathlib.Path(N_test[idx]).stem for idx in idxs_sample]\n",
    "    df = pd.DataFrame(data=probas_sample, index=short_names, columns=label_strs)\n",
    "\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html\n",
    "    return df.style.highlight_max(axis=1).format('{:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_C(logreg_pipe, params_best, Cs.min(), F_train, y_train, F_test, y_test, F_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_C(logreg_pipe, params_best, Cs.max(), F_train, y_train, F_test, y_test, F_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
