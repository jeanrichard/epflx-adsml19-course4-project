{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4 - Project - Part 7: Dense network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top-7\"></a>\n",
    "This notebook is concerned with *Part 7: Dense network*.\n",
    "\n",
    "**Contents:**\n",
    "* [Step 0: Loading data](#step-7.0)\n",
    "* [Step 1: 1-layer dense network](#step-7.1)\n",
    "* [Step 2: 2-layer dense network](#step-7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Loading data<a name=\"step-7.0\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y nomkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library.\n",
    "import pathlib\n",
    "import typing as T\n",
    "\n",
    "# 3rd party.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the datasets with the extracted high-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Dataset: train\n",
      "data: shape=(280, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(280,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(280,), dtype=<U19\n",
      "features: shape=(280, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: valid\n",
      "data: shape=(139, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(139,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(139,), dtype=<U19\n",
      "features: shape=(139, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: test\n",
      "data: shape=(50, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(50,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(50,), dtype=<U19\n",
      "features: shape=(50, 1280), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "separator = ''.center(80, '-')\n",
    "\n",
    "path_train = pathlib.Path.cwd() / 'data' / 'swissroads-features-train.npz'\n",
    "data_train = utils.load(path_train)\n",
    "print(separator)\n",
    "print(f'Dataset: train\\n{utils.info(data_train)}')\n",
    "\n",
    "path_valid = pathlib.Path.cwd() / 'data' / 'swissroads-features-valid.npz'\n",
    "data_valid = utils.load(path_valid)\n",
    "print(separator)\n",
    "print(f'Dataset: valid\\n{utils.info(data_valid)}')\n",
    "\n",
    "path_test = pathlib.Path.cwd() / 'data' / 'swissroads-features-test.npz'\n",
    "data_test = utils.load(path_test)\n",
    "print(separator)\n",
    "print(f'Dataset: test\\n{utils.info(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strs = data_train['label_strs']  # Same for all data sets.\n",
    "assert (\n",
    "    np.all(data_train['label_strs'] == data_valid['label_strs']) and\n",
    "    np.all(data_train['label_strs'] == data_test['label_strs'])\n",
    ")\n",
    "\n",
    "X_train = data_train['data']\n",
    "y_train = data_train['label_idxs']\n",
    "F_train = data_train['features']\n",
    "N_train = data_train['names']\n",
    "\n",
    "X_valid = data_valid['data']\n",
    "y_valid = data_valid['label_idxs']\n",
    "F_valid = data_valid['features']\n",
    "N_valid = data_train['names']\n",
    "\n",
    "X_test = data_test['data']\n",
    "y_test = data_test['label_idxs']\n",
    "F_test = data_test['features']\n",
    "N_test = data_test['names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fix the seed for the PRNGs in order to try to make computations deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 1-layer dense network<a name=\"step-7.1\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an utility function to build our Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of classes.\n",
    "n_classes = len(label_strs)\n",
    "    \n",
    "\n",
    "def build_model1(dropout_rate: float = 0.0,\n",
    "                l2_alpha: float = 0.01):\n",
    "    \"\"\"\\\n",
    "    Builds, compiles and returns a Keras model.\n",
    "    \n",
    "    .. seealso:: https://keras.io/scikit-learn-api/\n",
    "    \"\"\"\n",
    "    # Create model.\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add input layer.\n",
    "    if dropout_rate:\n",
    "        # Add drop-out layer.\n",
    "        model.add(layers.Dropout(dropout_rate, seed=RANDOM_SEED, input_shape=(1280,)))\n",
    "    else:\n",
    "        model.add(layers.InputLayer(input_shape=(1280,)))\n",
    "\n",
    "    # Add output layer.\n",
    "    model.add(layers.Dense(\n",
    "        units=n_classes, activation=activations.softmax,\n",
    "        kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=RANDOM_SEED),\n",
    "        kernel_regularizer=keras.regularizers.l2(l=l2_alpha)\n",
    "    ))\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(),  # use defaults\n",
    "        loss=losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Regarding regularization:\n",
    "* *Dropout.* We were not sure whether it is a good idea to use dropout on the input layer. We decided to do it after reading online that this is done in the article that introduced the technique ([Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html)).\n",
    "* *L2 regularization.* We add L2 regularization.\n",
    "* *Early stopping.* We were not sure whether we should use early stopping or not. We decided not to do it after reading some contradictory opinions online in the context of grid-search (e.g. [this post](https://stackoverflow.com/a/48139341) on StackOverflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the grid of values to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_rates = [0.1, 1/4, 1/3, 1/2]\n",
    "\n",
    "dropout_rates = (\n",
    "    # No dropout.\n",
    "    [0.0] + \n",
    "    # Input layer.\n",
    "    nonzero_rates\n",
    ")\n",
    "\n",
    "l2_alphas = [0.0, 0.01]  # disable: 0.0, default: 0.01\n",
    "\n",
    "param_grid1 = {\n",
    "    'dropout_rate': dropout_rates,\n",
    "    'l2_alpha': l2_alphas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Regarding the grid-search: Given the ratio of training data (280 samples) to validation data (139 samples), using a ``GridSearchCV`` would require at least 3 folds in order to train the model on folds of a size equivalent to the training set. This would take more time than we are willing to allocate. On the other hand, we very much like the convenience offered by ``GridSearchCV`` (parallelism, automatic refitting, results easy to convert to a data-frame, etc.). It turns out that by manually providing the indices of the training and test sets we can use ``GridSearCV`` on a single fold. Since they are already properly \"stratified\" (similar distribution of classes), we can use the training and validation sets as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_large = np.concatenate([X_train, X_valid])\n",
    "y_train_large = np.concatenate([y_train, y_valid])\n",
    "F_train_large = np.concatenate([F_train, F_valid])\n",
    "N_train_large = np.concatenate([N_train, N_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make computations deterministic.\n",
    "utils.reset_seeds()\n",
    "\n",
    "model1 = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model1)\n",
    "\n",
    "# Compute the weight of each class.\n",
    "class_weight = utils.get_class_weight(y_train_large)\n",
    "\n",
    "# Compute the indices of the training and validation sets.\n",
    "idx_train = np.arange(0, len(y_train))\n",
    "idx_test = np.arange(len(y_train), len(y_train) + len(y_valid))\n",
    "cv = [(idx_train, idx_test)]\n",
    "\n",
    "nn_gscv1 = GridSearchCV(model1, param_grid1, n_jobs=-1, iid=False, refit=True, cv=cv, return_train_score=True)\n",
    "\n",
    "# Fit/evaluate the estimator.\n",
    "nn_gscv1.fit(F_train_large, y_train_large,\n",
    "             batch_size=32, epochs=50, verbose=0,\n",
    "             shuffle=True, class_weight=class_weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the results to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_dropout_rate</th>\n",
       "      <th>param_l2_alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_dropout_rate param_l2_alpha  mean_test_score  std_test_score  \\\n",
       "7           0.333333           0.01         0.920863             0.0   \n",
       "0                  0              0         0.906475             0.0   \n",
       "1                  0           0.01         0.906475             0.0   \n",
       "3                0.1           0.01         0.906475             0.0   \n",
       "4               0.25              0         0.906475             0.0   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "7               1.0              0.0  \n",
       "0               1.0              0.0  \n",
       "1               1.0              0.0  \n",
       "3               1.0              0.0  \n",
       "4               1.0              0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report1 = (\n",
    "    pd\n",
    "    .DataFrame(nn_gscv1.cv_results_)\n",
    "    .sort_values(by='mean_test_score', ascending=False)\n",
    ")\n",
    "df_report1[['param_dropout_rate', 'param_l2_alpha', 'mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the accuracy of the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step\n",
      "test accuracy: 90.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = nn_gscv1.best_estimator_.score(F_test, y_test)\n",
    "print(f'test accuracy: {accuracy_test * 100:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.3333333333333333, 'l2_alpha': 0.01}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_gscv1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the result.\n",
    "desc = ', '.join([f'{key}={nn_gscv1.best_params_[key]}' for key in ['dropout_rate', 'l2_alpha']])\n",
    "utils.persist_result('1-layer nn', 'part-07-a', desc, accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We accidentally noticed that by manually refitting the model on the extended training set a few times we were able to get an accuracy on the test set that varies between 0.90 and 0.92 (even 0.94). The fact that there are differences is probably due to the fact that the PRNGs are in a a different state each time. The amplitude of the differences is probably due to the fact that we are dealing with very small training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "min (max) score: 0.9000000047683716 (0.9199999928474426)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for _ in range(10):\n",
    "    model1 = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model1)\n",
    "    model1.set_params(**nn_gscv1.best_params_)\n",
    "    model1.fit(\n",
    "        F_train_large, y_train_large,\n",
    "        batch_size=32, epochs=50, verbose=0,\n",
    "        shuffle=True, class_weight=class_weight)\n",
    "    scores.append(model1.score(F_test, y_test))\n",
    "print(f'min (max) score: {np.min(scores)} ({np.max(scores)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 2-layer dense network<a name=\"step-7.2\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an utility function to build our Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(hidden_n_neurons: int,\n",
    "                 dropout_rate: T.Tuple[float, float] = (0.0, 0.0),\n",
    "                 l2_alpha: float = 0.01):\n",
    "    \"\"\"\\\n",
    "    Builds, compiles and returns a Keras model.\n",
    "    \n",
    "    .. seealso:: https://keras.io/scikit-learn-api/\n",
    "    \"\"\"\n",
    "    dropout_rate_input, dropout_rate_hidden = dropout_rate\n",
    "    \n",
    "    # Create model.\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add input layer.\n",
    "    if dropout_rate_input:\n",
    "        # Add drop-out layer.\n",
    "        model.add(layers.Dropout(dropout_rate_input, seed=RANDOM_SEED, input_shape=(1280,)))\n",
    "    else:\n",
    "        model.add(layers.InputLayer(input_shape=(1280,)))\n",
    "        \n",
    "    # Add hidden layer.\n",
    "    if dropout_rate_hidden:\n",
    "        # Add drop-out layer.\n",
    "        model.add(layers.Dropout(dropout_rate_hidden, seed=RANDOM_SEED))\n",
    "    model.add(layers.Dense(\n",
    "        units=hidden_n_neurons, activation=activations.relu,\n",
    "        kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=RANDOM_SEED),\n",
    "        kernel_regularizer=keras.regularizers.l2(l=l2_alpha)))\n",
    "\n",
    "    # Add output layer.\n",
    "    model.add(layers.Dense(\n",
    "        units=n_classes, activation=activations.softmax,\n",
    "        kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=RANDOM_SEED),\n",
    "        kernel_regularizer=keras.regularizers.l2(l=l2_alpha)\n",
    "    ))\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(),  # use defaults\n",
    "        loss=losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']  # cannot use metrics.sparse_categorical_accuracy\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the grid of values to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_rates = [0.1, 1/4, 1/3, 1/2]\n",
    "\n",
    "dropout_rates = (\n",
    "    # No dropout.\n",
    "    [(0.0, 0.0)] +\n",
    "    # Hidden layer only.\n",
    "    list(itertools.product([0.0], nonzero_rates)) +\n",
    "    # Input and hidden layers (same rate).\n",
    "    list(zip(nonzero_rates, nonzero_rates))\n",
    ")\n",
    "\n",
    "hidden_n_neurons = [10, 25, 50, 75, 100, 250, 500]\n",
    "\n",
    "l2_alphas = [0.0, 0.01]  # disable: 0.0, default: 0.01\n",
    "\n",
    "param_grid2 = {\n",
    "    'dropout_rate': dropout_rates,\n",
    "    'hidden_n_neurons': hidden_n_neurons,\n",
    "    'l2_alpha': l2_alphas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make computations deterministic.\n",
    "utils.reset_seeds()\n",
    "\n",
    "model2 = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model2)\n",
    "\n",
    "# Compute the weight of each class.\n",
    "class_weight = utils.get_class_weight(y_train_large)\n",
    "\n",
    "# Compute the indices of the training and validation sets.\n",
    "idx_train = np.arange(0, len(y_train))\n",
    "idx_test = np.arange(len(y_train), len(y_train) + len(y_valid))\n",
    "cv = [(idx_train, idx_test)]\n",
    "\n",
    "nn_gscv2 = GridSearchCV(model2, param_grid2, n_jobs=-1, iid=False, refit=True, cv=cv, return_train_score=True)\n",
    "\n",
    "# Fit/evaluate the estimator.\n",
    "nn_gscv2.fit(F_train_large, y_train_large,\n",
    "             batch_size=32, epochs=50, verbose=0,\n",
    "             shuffle=True, class_weight=class_weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the results to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_dropout_rate</th>\n",
       "      <th>param_hidden_n_neurons</th>\n",
       "      <th>param_l2_alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(0.0, 0.1)</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>(0.5, 0.5)</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(0.25, 0.25)</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(0.3333333333333333, 0.3333333333333333)</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>(0.5, 0.5)</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           param_dropout_rate param_hidden_n_neurons  \\\n",
       "26                                 (0.0, 0.1)                    500   \n",
       "122                                (0.5, 0.5)                    250   \n",
       "96                               (0.25, 0.25)                    500   \n",
       "98   (0.3333333333333333, 0.3333333333333333)                     10   \n",
       "114                                (0.5, 0.5)                     25   \n",
       "\n",
       "    param_l2_alpha  mean_test_score  std_test_score  mean_train_score  \\\n",
       "26               0         0.942446             0.0               1.0   \n",
       "122              0         0.942446             0.0               1.0   \n",
       "96               0         0.942446             0.0               1.0   \n",
       "98               0         0.935252             0.0               1.0   \n",
       "114              0         0.935252             0.0               1.0   \n",
       "\n",
       "     std_train_score  \n",
       "26               0.0  \n",
       "122              0.0  \n",
       "96               0.0  \n",
       "98               0.0  \n",
       "114              0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report2 = (\n",
    "    pd\n",
    "    .DataFrame(nn_gscv2.cv_results_)\n",
    "    .sort_values(by='mean_test_score', ascending=False)\n",
    ")\n",
    "df_report2[['param_dropout_rate', 'param_hidden_n_neurons', 'param_l2_alpha', 'mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the accuracy of the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 5ms/step\n",
      "test accuracy: 94.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = nn_gscv2.best_estimator_.score(F_test, y_test)\n",
    "print(f'test accuracy: {accuracy_test * 100:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': (0.0, 0.1), 'hidden_n_neurons': 500, 'l2_alpha': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_gscv2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the result.\n",
    "desc = ', '.join([f'{key}={nn_gscv2.best_params_[key]}' for key in ['dropout_rate', 'hidden_n_neurons', 'l2_alpha']])\n",
    "utils.persist_result('2-layer nn', 'part-07-b', desc, accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
