{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4 - Project - Part 7: Dense network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top-7\"></a>\n",
    "This notebook is concerned with *Part 7: Dense network*.\n",
    "\n",
    "**Contents:**\n",
    "* [Step 0: Loading data](#step-7.0)\n",
    "* [Step 1: 1-layer dense network](#step-7.1)\n",
    "* [Step 2: 2-layer dense network](#step-7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Loading data<a name=\"step-7.0\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training set with the extracted high-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library.\n",
    "import os\n",
    "import pathlib\n",
    "import typing as T\n",
    "\n",
    "# 3rd party.\n",
    "import numpy as np\n",
    "\n",
    "# Project.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Dataset: train\n",
      "data: shape=(280, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(280,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(280,), dtype=<U19\n",
      "features: shape=(280, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: valid\n",
      "data: shape=(139, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(139,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(139,), dtype=<U19\n",
      "features: shape=(139, 1280), dtype=float32\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: test\n",
      "data: shape=(50, 224, 224, 3), dtype=float32\n",
      "label_idxs: shape=(50,), dtype=int64\n",
      "label_strs: shape=(6,), dtype=<U10\n",
      "names: shape=(50,), dtype=<U19\n",
      "features: shape=(50, 1280), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "separator = ''.center(80, '-')\n",
    "\n",
    "path_train = pathlib.Path.cwd() / 'data' / 'swissroads-features-train.npz'\n",
    "data_train = utils.load(path_train)\n",
    "print(separator)\n",
    "print(f'Dataset: train\\n{utils.info(data_train)}')\n",
    "\n",
    "path_valid = pathlib.Path.cwd() / 'data' / 'swissroads-features-valid.npz'\n",
    "data_valid = utils.load(path_valid)\n",
    "print(separator)\n",
    "print(f'Dataset: valid\\n{utils.info(data_valid)}')\n",
    "\n",
    "path_test = pathlib.Path.cwd() / 'data' / 'swissroads-features-test.npz'\n",
    "data_test = utils.load(path_test)\n",
    "print(separator)\n",
    "print(f'Dataset: test\\n{utils.info(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strs = data_train['label_strs']  # Same for all data sets.\n",
    "assert (\n",
    "    np.all(data_train['label_strs'] == data_valid['label_strs']) and\n",
    "    np.all(data_train['label_strs'] == data_test['label_strs'])\n",
    ")\n",
    "\n",
    "X_train = data_train['data']\n",
    "y_train = data_train['label_idxs']\n",
    "F_train = data_train['features']\n",
    "N_train = data_train['names']\n",
    "\n",
    "X_valid = data_valid['data']\n",
    "y_valid = data_valid['label_idxs']\n",
    "F_valid = data_valid['features']\n",
    "N_valid = data_train['names']\n",
    "\n",
    "X_test = data_test['data']\n",
    "y_test = data_test['label_idxs']\n",
    "F_test = data_test['features']\n",
    "N_test = data_test['names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 1-layer dense network<a name=\"step-7.1\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, try with neural networks\n",
    "\n",
    "1-layer dense network i.e. no hidden layer, just the input and output ones\n",
    "2-layer dense network i.e. one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Create and com.\n",
    "n_classes = len(label_strs)\n",
    "    \n",
    "\n",
    "def build_model(dropout_rate: float = 0.0, l2_alpha: float = 0.01):\n",
    "    \"\"\"\\\n",
    "    Build, compiles and returns a Keras model.\n",
    "    \n",
    "    .. seealso:: https://keras.io/scikit-learn-api/\n",
    "    \"\"\"\n",
    "    # Create model.\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # This makes building a network easier.\n",
    "    model.add(keras.layers.InputLayer(input_shape=(1280,)))\n",
    "\n",
    "    # Add drop-out layer.\n",
    "    model.add(keras.layers.Dropout(dropout_rate, seed=RANDOM_SEED))\n",
    "\n",
    "    # Add output layer.\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=n_classes, activation=activations.softmax,\n",
    "        kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=RANDOM_SEED),\n",
    "        kernel_regularizer=keras.regularizers.l2(l=l2_alpha)\n",
    "    ))\n",
    "\n",
    "    # Print network summary.\n",
    "#     model.summary()\n",
    "    \n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(),  # use defaults\n",
    "        loss=losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']  # cannot use metrics.sparse_categorical_accuracy\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of values to search.\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.0, 0.1],#, 0.2, 0.3, 0.4, 0.5],  # disable: 0.0\n",
    "    'l2_alpha': [0.0, 0.001, 0.01, 0.1],  # disable: 0.0, default: 0.01\n",
    "}\n",
    "\n",
    "# End training when accuracy stops improving (optional).\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# gscv = GridSearchCV(model, param_grid, iid=False, cv=3)\n",
    "# gscv.fit(F_train, y_train, batch_size=32, epochs=100, verbose=0,\n",
    "#          callbacks=[early_stopping],\n",
    "#          validation_data=(F_valid, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7070707070707071,\n",
       " 1: 0.7291666666666666,\n",
       " 2: 0.9150326797385621,\n",
       " 3: 1.4583333333333333,\n",
       " 4: 1.1111111111111112,\n",
       " 5: 1.8666666666666667}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_classes = np.unique(y_train)\n",
    "class_weight_ = dict(zip(y_classes, compute_class_weight('balanced', y_classes, y_train)))\n",
    "class_weight_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 0s 140us/step\n",
      "139/139 [==============================] - 0s 155us/step\n",
      "280/280 [==============================] - 0s 125us/step\n",
      "139/139 [==============================] - 0s 157us/step\n",
      "280/280 [==============================] - 0s 154us/step\n",
      "139/139 [==============================] - 0s 172us/step\n",
      "280/280 [==============================] - 0s 115us/step\n",
      "139/139 [==============================] - 0s 143us/step\n",
      "280/280 [==============================] - 0s 120us/step\n",
      "139/139 [==============================] - 0s 144us/step\n",
      "280/280 [==============================] - 0s 116us/step\n",
      "139/139 [==============================] - 0s 148us/step\n",
      "280/280 [==============================] - 0s 314us/step\n",
      "139/139 [==============================] - 0s 147us/step\n",
      "280/280 [==============================] - 0s 202us/step\n",
      "139/139 [==============================] - 0s 149us/step\n"
     ]
    }
   ],
   "source": [
    "gs_results = []\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for i, params_dict in enumerate(grid, start=1):\n",
    "    print(f'\\rconfiguration: {i}', end='', flush=True)\n",
    "    \n",
    "    # Set the parameters.\n",
    "    model.set_params(**params_dict)\n",
    "    \n",
    "    # Fit the clasifier to the data.\n",
    "    history = model.fit(\n",
    "        F_train, y_train, batch_size=32, epochs=100, verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=(F_valid, y_valid), shuffle=True, class_weight=class_weight_)\n",
    "    \n",
    "    \n",
    "    # Store the scores.\n",
    "    gs_result = params_dict\n",
    "    # Due to dropout, etc. this will not be exactly the same as mode.score(F_train, y_train).\n",
    "#     gs_result['train_accuracy'] = history.history['acc'][-1]\n",
    "    gs_result['train_accuracy'] = model.score(F_train, y_train)\n",
    "#     gs_result['valid_accuracy'] = history.history['val_acc'][-1]\n",
    "    gs_result['valid_accuracy'] = model.score(F_valid, y_valid)\n",
    "    \n",
    "    # Save result.\n",
    "    gs_results.append(gs_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>l2_alpha</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dropout_rate  l2_alpha  train_accuracy  valid_accuracy\n",
       "7           0.1     0.100             1.0        0.913669\n",
       "4           0.1     0.000             1.0        0.906475\n",
       "6           0.1     0.010             1.0        0.906475\n",
       "2           0.0     0.010             1.0        0.906475\n",
       "1           0.0     0.001             1.0        0.899281"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to a data frame.\n",
    "df_results = (pd\n",
    "              .DataFrame(gs_results)\n",
    "              .sort_values(by='valid_accuracy', ascending=False)\n",
    "             )\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The best result is 91.3 % accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use a random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part it makes sense to merge the training and the validation sets for cross-validation (since we would not make use of the validation set otherwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 2-layer dense network<a name=\"step-7.2\"></a> ([top](#top-7))\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(input_shape=(1280,), n_hidden=1, n_hidden_neurons=30, n_classes=n_classes, dropout_rate: float = 0.0, l2_alpha: float = 0.0):\n",
    "    \"\"\"\\\n",
    "    Build, compiles and returns a Keras model.\n",
    "    \n",
    "    .. seealso:: https://keras.io/scikit-learn-api/\n",
    "    \"\"\"\n",
    "    # Create model.\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add drop-out layer.\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(1280,), seed=RANDOM_SEED))\n",
    "\n",
    "    # Add output layer.\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=n_classes, activation=activations.softmax,\n",
    "        kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=RANDOM_SEED),\n",
    "        kernel_regularizer=keras.regularizers.l2(l=l2_alpha)\n",
    "    ))\n",
    "\n",
    "    # Print network summary.\n",
    "#     model.summary()\n",
    "    \n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(),  # use defaults\n",
    "        loss=losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']  # cannot use metrics.sparse_categorical_accuracy\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to tune the regularization strength of the logistic regression classifier with cross-validated grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We have imbalanced classes (e.g. 22.63% bike vs. 8.83% van)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = (pd\n",
    " .DataFrame(data=pd.Series(data=y_train_large).value_counts(), columns=['count'])\n",
    " .set_index(label_strs)\n",
    ")\n",
    "df_counts['fraction'] = df_counts['count'] / df_counts['count'].sum()\n",
    "df_counts.style.format({'fraction': '{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the estimator.\n",
    "svm_pipe = Pipeline([\n",
    "    ('svm', LinearSVC(random_state=RANDOM_STATE)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-4, 4, num=2 * 8 + 1)  # C defaults to 1.0.\n",
    "gammas = [0.01, 0.1, 1.0, 10.0, 'scale']\n",
    "\n",
    "# Setup the cross-validated grid search.\n",
    "grid = [\n",
    "    # LinearSVC (minize: squared hinge loss, strategy: one-vs-rest)\n",
    "    {\n",
    "        'svm__C': Cs,\n",
    "        'svm__class_weight':[None, 'balanced']\n",
    "    },\n",
    "    # SVC (kernel: linear, minimize: hinge loss, strategy: one-vs-one)\n",
    "    {\n",
    "        'svm': [SVC(random_state=RANDOM_STATE)],\n",
    "        'svm__kernel': ['linear'],\n",
    "        'svm__C': Cs,\n",
    "        'svm__class_weight':[None, 'balanced']\n",
    "    },\n",
    "    # SVC (kernel: RBF, minimize: hinge loss, strategy: one-vs-one)\n",
    "    {\n",
    "        'svm': [SVC(random_state=RANDOM_STATE)],\n",
    "        'svm__kernel': ['rbf'],\n",
    "        'svm__C': Cs,\n",
    "        'svm__gamma': gammas,\n",
    "        'svm__class_weight':[None, 'balanced']\n",
    "    }\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=RANDOM_STATE)\n",
    "svm_gscv = GridSearchCV(svm_pipe, grid, n_jobs=-1, iid=False, refit=True, cv=cv, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/evaluate the estimator.\n",
    "svm_gscv.fit(F_train_large, y_train_large);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results in a data frame.\n",
    "df_results = (pd\n",
    "    .DataFrame({\n",
    "        'svm': svm_gscv.cv_results_['param_svm'],\n",
    "        'kernel': svm_gscv.cv_results_['param_svm__kernel'],\n",
    "        'C': svm_gscv.cv_results_['param_svm__C'],\n",
    "        'gamma': svm_gscv.cv_results_['param_svm__gamma'],\n",
    "        'class_weight': svm_gscv.cv_results_['param_svm__class_weight'],\n",
    "        'mean_train_score': svm_gscv.cv_results_['mean_train_score'],\n",
    "        'mean_test_score': svm_gscv.cv_results_['mean_test_score'],\n",
    "        'std_test_score': svm_gscv.cv_results_['std_test_score'],\n",
    "        'params': svm_gscv.cv_results_['params']\n",
    "    })\n",
    "    .sort_values(by='mean_test_score', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
